{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a57583-e009-44b0-8c74-97efc9828b32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-01 22:17:28 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 09-01 22:17:29 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 09-01 22:17:38 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 09-01 22:17:38 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 09-01 22:17:39 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='llava-hf/llava-v1.6-mistral-7b-hf', speculative_config=None, tokenizer='llava-hf/llava-v1.6-mistral-7b-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=llava-hf/llava-v1.6-mistral-7b-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 09-01 22:17:40 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f7f0b690370>\n",
      "INFO 09-01 22:17:40 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 09-01 22:17:40 [cuda.py:221] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-01 22:17:42 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-01 22:17:42 [gpu_model_runner.py:1329] Starting to load model llava-hf/llava-v1.6-mistral-7b-hf...\n",
      "INFO 09-01 22:17:43 [config.py:3614] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]\n",
      "INFO 09-01 22:17:43 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31fb0c194e074e1d91ad33ad867c6571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-01 22:17:46 [loader.py:458] Loading weights took 2.87 seconds\n",
      "INFO 09-01 22:17:46 [gpu_model_runner.py:1347] Model loading took 14.0786 GiB and 3.747050 seconds\n",
      "INFO 09-01 22:17:46 [gpu_model_runner.py:1620] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 6 image items of the maximum feature size.\n",
      "INFO 09-01 22:17:53 [backends.py:420] Using cache directory: /home/runai-home/.cache/vllm/torch_compile_cache/2be1a315ce/rank_0_0 for vLLM's torch.compile\n",
      "INFO 09-01 22:17:53 [backends.py:430] Dynamo bytecode transform time: 6.07 s\n",
      "INFO 09-01 22:17:58 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 4.708 s\n",
      "INFO 09-01 22:17:59 [monitor.py:33] torch.compile takes 6.07 s in total\n",
      "INFO 09-01 22:18:00 [kv_cache_utils.py:634] GPU KV cache size: 447,104 tokens\n",
      "INFO 09-01 22:18:00 [kv_cache_utils.py:637] Maximum concurrency for 4,096 tokens per request: 109.16x\n",
      "INFO 09-01 22:18:26 [gpu_model_runner.py:1686] Graph capturing finished in 26 secs, took 0.51 GiB\n",
      "INFO 09-01 22:18:26 [core.py:159] init engine (profile, create kv cache, warmup model) took 39.86 seconds\n",
      "INFO 09-01 22:18:26 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-01 22:18:28] INFO modeling.py:989: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a8b0b9a82143998d57af1bbd829738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-01 22:18:30] WARNING big_modeling.py:435: Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "ROOT_DIR = '../src'\n",
    "\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "from endopoint.models import LLaVAModel\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize the model\n",
    "model = LLaVAModel(\n",
    "  model_name=\"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "  use_vllm=True,  # Use vLLM for faster inference (or False for transformers)\n",
    "  max_tokens=100,\n",
    "  temperature=0.0\n",
    ")\n",
    "\n",
    "# # Load an image\n",
    "# image = Image.open(\"path/to/image.jpg\")\n",
    "\n",
    "# # Simple query\n",
    "# response = model((image, \"What do you see in this image?\"))\n",
    "# print(response)\n",
    "\n",
    "# # With system prompt\n",
    "# response = model(\n",
    "#   (image, \"Identify all visible organs\"),\n",
    "#   system_prompt=\"You are a medical image analyst\"\n",
    "# )\n",
    "\n",
    "# For CholecSeg8k Dataset\n",
    "\n",
    "from endopoint.datasets import build_dataset\n",
    "from endopoint.models import LLaVAModel\n",
    "\n",
    "# Load dataset and model\n",
    "dataset = build_dataset(\"cholecseg8k\")\n",
    "model = LLaVAModel(\n",
    "  model_name=\"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "  use_vllm=False,  # Use transformers if vLLM has issues\n",
    "  max_tokens=100\n",
    ")\n",
    "\n",
    "# Get image from dataset\n",
    "example = dataset.get_example(\"train\", 0)\n",
    "image = example['image']\n",
    "\n",
    "# Organ detection\n",
    "organs = [\"Liver\", \"Gallbladder\", \"Fat\", \"Grasper\"]\n",
    "prompt = f\"Which of these organs are visible: {', '.join(organs)}? Respond with JSON.\"\n",
    "response = model((image, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19d5017-7ee3-434b-9272-a705e73f19ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"visible_organs\": [\"Liver\", \"Gallbladder\", \"Fat\"],\\n  \"visible_tools\": [\"Grasper\"]\\n}\\n```'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfb15217-b11e-4d4b-9e44-3f3483d84a94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-01 21:47:33 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 09-01 21:47:43 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 09-01 21:47:43 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 09-01 21:47:44 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='llava-hf/llava-v1.6-mistral-7b-hf', speculative_config=None, tokenizer='llava-hf/llava-v1.6-mistral-7b-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=llava-hf/llava-v1.6-mistral-7b-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 09-01 21:47:44 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f3107dca350>\n",
      "INFO 09-01 21:47:45 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 09-01 21:47:45 [cuda.py:221] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-01 21:47:47 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-01 21:47:47 [gpu_model_runner.py:1329] Starting to load model llava-hf/llava-v1.6-mistral-7b-hf...\n",
      "INFO 09-01 21:47:47 [config.py:3614] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]\n",
      "INFO 09-01 21:47:47 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c36d4c8635d456e97be8cb92bee720b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-01 21:47:51 [loader.py:458] Loading weights took 2.80 seconds\n",
      "INFO 09-01 21:47:51 [gpu_model_runner.py:1347] Model loading took 14.0786 GiB and 3.944231 seconds\n",
      "INFO 09-01 21:47:51 [gpu_model_runner.py:1620] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 6 image items of the maximum feature size.\n",
      "INFO 09-01 21:47:57 [backends.py:420] Using cache directory: /home/runai-home/.cache/vllm/torch_compile_cache/d42b4cba62/rank_0_0 for vLLM's torch.compile\n",
      "INFO 09-01 21:47:57 [backends.py:430] Dynamo bytecode transform time: 6.17 s\n",
      "INFO 09-01 21:48:00 [backends.py:136] Cache the graph of shape None for later use\n",
      "INFO 09-01 21:48:21 [backends.py:148] Compiling a graph for general shape takes 23.40 s\n",
      "INFO 09-01 21:48:35 [monitor.py:33] torch.compile takes 29.56 s in total\n",
      "INFO 09-01 21:48:35 [kv_cache_utils.py:634] GPU KV cache size: 445,056 tokens\n",
      "INFO 09-01 21:48:35 [kv_cache_utils.py:637] Maximum concurrency for 4,096 tokens per request: 108.66x\n",
      "INFO 09-01 21:49:03 [gpu_model_runner.py:1686] Graph capturing finished in 28 secs, took 0.51 GiB\n",
      "INFO 09-01 21:49:03 [core.py:159] init engine (profile, create kv cache, warmup model) took 72.38 seconds\n",
      "INFO 09-01 21:49:03 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d565cc1588254c5c9365e8a17a9e3190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM output: The image shows a nighttime view of London, with the iconic Big Ben clock tower illuminated in the background. The photo captures the motion blur of the city's traffic, with cars appearing as streaks, creating a sense of movement and activity. The sky is dark, and there are stars visible, indicating it might be a clear night. The overall scene conveys a bustling urban environment. \n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "def run_llava_next():\n",
    "    llm = LLM(model=\"llava-hf/llava-v1.6-mistral-7b-hf\", max_model_len=4096)\n",
    "\n",
    "    prompt = \"[INST] <image>\\nWhat is shown in this image? [/INST]\"\n",
    "    url = \"https://h2o-release.s3.amazonaws.com/h2ogpt/bigben.jpg\"\n",
    "    image = Image.open(BytesIO(requests.get(url).content))\n",
    "    sampling_params = SamplingParams(temperature=0.8,\n",
    "                                     top_p=0.95,\n",
    "                                     max_tokens=100)\n",
    "\n",
    "    outputs = llm.generate(\n",
    "        {\n",
    "            \"prompt\": prompt,\n",
    "            \"multi_modal_data\": {\n",
    "                \"image\": image\n",
    "            }\n",
    "        },\n",
    "        sampling_params=sampling_params)\n",
    "\n",
    "    generated_text = \"\"\n",
    "    for o in outputs:\n",
    "        generated_text += o.outputs[0].text\n",
    "\n",
    "    print(f\"LLM output:{generated_text}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_llava_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9849cf-2430-402f-9564-099bf499f922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
