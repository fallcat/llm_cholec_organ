{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "146d68f4-7840-4ba7-9219-62a0cca176a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "from collections import namedtuple\n",
    "from typing import Optional, Union, List\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "import kornia\n",
    "import time\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "\n",
    "# The kinds of splits we can do\n",
    "SPLIT_TYPES = ['train', 'test']\n",
    "\n",
    "# Splitting images by video source\n",
    "VIDEO_GLOBS_PUBLIC = \\\n",
    "      [f\"cholec80_video{i:02d}_*\" for i in range(1,81)] \\\n",
    "    + [f\"M2CCAI2016_video{i}_*\" for i in range(81,122)]\n",
    "\n",
    "VIDEO_GLOBS_PRIVATE = \\\n",
    "      [f\"AdnanSet_LC_{i}_*\" for i in range(1,165)] \\\n",
    "    + [f\"AminSet_LC_{i}_*\" for i in range(1,11)] \\\n",
    "    + [\"HokkaidoSet_LC_1_*\", \"HokkaidoSet_LC_2_*\"] \\\n",
    "    + [f\"UTSWSet_Case_{i}_*\" for i in range(1,13)] \\\n",
    "    + [f\"WashUSet_LC_01_*\"]\n",
    "\n",
    "VIDEO_GLOBS_DICT = {\n",
    "    'public': VIDEO_GLOBS_PUBLIC,\n",
    "    'private': VIDEO_GLOBS_PRIVATE\n",
    "}\n",
    "#\n",
    "class CholecGonogoDataset(Dataset):\n",
    "    \n",
    "    gonogo_names: str = [\n",
    "        \"Background\",\n",
    "        \"Go\",\n",
    "        \"Nogo\"\n",
    "    ]\n",
    "\n",
    "    organ_names: str = [\n",
    "        \"Background\",\n",
    "        \"Liver\",\n",
    "        \"Gallbladder\",\n",
    "        \"Hepatocystic Triangle\"\n",
    "    ]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "         data_dir: str,\n",
    "         images_dir: str = \"images\",\n",
    "         gonogo_labels_dir: str = \"gonogo_labels\",\n",
    "         organ_labels_dir: str = \"organ_labels\",\n",
    "         split: str = \"train\",\n",
    "         split_filepath: str = None,\n",
    "         video_globs: Union[str, list] = 'public',\n",
    "         train_ratio: float = 0.8,\n",
    "         image_height: float = 384,\n",
    "         image_width: float = 640,\n",
    "         train_angle_max: float = 60.0,\n",
    "         image_transforms = None,\n",
    "         label_transforms = None,\n",
    "         download: bool = False,\n",
    "         gen_seed: int = 1234,\n",
    "         augmentations: bool = False,\n",
    "         pretransform: bool = False\n",
    "    ):\n",
    "        if download:\n",
    "            raise ValueError(\"download not implemented\")\n",
    "            \n",
    "        # assert split in ['train', 'test']\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.images_dir = os.path.join(data_dir, images_dir)\n",
    "        self.gonogo_labels_dir = os.path.join(data_dir, gonogo_labels_dir)\n",
    "        self.organ_labels_dir = os.path.join(data_dir, organ_labels_dir)\n",
    "\n",
    "        assert os.path.isdir(self.images_dir)\n",
    "        assert os.path.isdir(self.gonogo_labels_dir)\n",
    "        assert os.path.isdir(self.organ_labels_dir)\n",
    "        assert split in SPLIT_TYPES\n",
    "        self.split = split\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "        gen = torch.Generator()\n",
    "        gen.manual_seed(gen_seed)\n",
    "        \n",
    "        # Use existing video globs or new ones\n",
    "        if isinstance(video_globs, str):\n",
    "            if video_globs in VIDEO_GLOBS_DICT:\n",
    "                VIDEO_GLOBS = VIDEO_GLOBS_DICT[video_globs]\n",
    "            else:\n",
    "                raise ValueError(f'video_globs {video_globs} does not exist, please pass in one of {VIDEO_GLOBS_DICT.keys()} or a list.')\n",
    "        elif isinstance(video_globs, list):\n",
    "            VIDEO_GLOBS = video_globs\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        self.video_globs = VIDEO_GLOBS\n",
    "        \n",
    "        # Split by the video source\n",
    "        if split_filepath is not None and os.path.isfile(split_filepath):\n",
    "            with open(split_filepath, 'rt') as input_file:\n",
    "                self.image_files = [line.strip() for line in input_file.readlines()]\n",
    "            \n",
    "        elif split == \"train\" or split == \"test\":\n",
    "            num_all, num_train = len(VIDEO_GLOBS), int(len(VIDEO_GLOBS) * train_ratio)\n",
    "            perm = torch.randperm(num_all, generator=gen)\n",
    "            idxs = perm[:num_train] if \"train\" in split else perm[num_train:]\n",
    "\n",
    "            image_files = []\n",
    "            for i in idxs:\n",
    "                # import pdb; pdb.set_trace()\n",
    "                image_files += [os.path.basename(path) for path in \\\n",
    "                    glob.glob(os.path.join(self.images_dir, VIDEO_GLOBS[i]))]\n",
    "            self.image_files = sorted(image_files)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        gen.manual_seed(gen_seed) # rotation seed shouldn't be affected by the split seed\n",
    "        # Random rotation angles used for the training data\n",
    "        self.train_angle_max = train_angle_max\n",
    "        self.random_angles = train_angle_max * (torch.rand(len(self.image_files)) * 2 - 1)\n",
    "\n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "\n",
    "        # Image transforms\n",
    "        if image_transforms is None:\n",
    "            self.image_transforms = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((image_height, image_width), antialias=True),\n",
    "            ])\n",
    "        else:\n",
    "            assert callable(image_transforms)\n",
    "            self.image_transforms = image_transforms\n",
    "\n",
    "        if label_transforms is None:\n",
    "            self.label_transforms = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((image_height, image_width), antialias=True)\n",
    "            ])\n",
    "        else:\n",
    "            assert callable(label_transforms)\n",
    "            self.label_transforms = label_transforms\n",
    "\n",
    "        self.pretransform = pretransform\n",
    "        if pretransform:\n",
    "            self.pretransform_data()\n",
    "\n",
    "    # import torch\n",
    "    # from tqdm import tqdm\n",
    "    # from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "    def pretransform_data(self, batch_size=8):\n",
    "        print('pretransforming data')\n",
    "        images = []\n",
    "        organs = []\n",
    "        gonogos = []\n",
    "\n",
    "        images_aug = []\n",
    "        organs_aug = []\n",
    "        gonogos_aug = []\n",
    "\n",
    "        # Iterate through dataset in batches\n",
    "        for batch_start in tqdm(range(0, len(self.image_files), batch_size)):\n",
    "            batch_end = min(batch_start + batch_size, len(self.image_files))\n",
    "            batch_images = []\n",
    "            batch_organs = []\n",
    "            batch_gonogos = []\n",
    "\n",
    "            # Load and preprocess each image in the batch\n",
    "            for idx in range(batch_start, batch_end):\n",
    "                image_file = os.path.join(self.images_dir, self.image_files[idx])\n",
    "                organ_label_file = os.path.join(self.organ_labels_dir, self.image_files[idx])\n",
    "                gonogo_label_file = os.path.join(self.gonogo_labels_dir, self.image_files[idx])\n",
    "\n",
    "                image = Image.open(image_file).convert(\"RGB\")\n",
    "                organ_label = Image.open(organ_label_file).convert(\"L\")\n",
    "                gonogo_label = Image.open(gonogo_label_file).convert(\"L\")\n",
    "\n",
    "                # Apply transforms and move to device\n",
    "                image = self.image_transforms(image)\n",
    "                organ_label = self.label_transforms(organ_label)\n",
    "                gonogo_label = self.label_transforms(gonogo_label)\n",
    "\n",
    "                batch_images.append(image)\n",
    "                batch_organs.append(organ_label)\n",
    "                batch_gonogos.append(gonogo_label)\n",
    "\n",
    "            # Stack images and labels into batch tensors\n",
    "            batch_images = torch.stack(batch_images)\n",
    "            batch_organs = torch.stack(batch_organs)\n",
    "            batch_gonogos = torch.stack(batch_gonogos)\n",
    "\n",
    "            # Collect batches\n",
    "            images.append(batch_images)\n",
    "            organs.append(batch_organs)\n",
    "            gonogos.append(batch_gonogos)\n",
    "\n",
    "            # Apply augmentations if enabled\n",
    "            if self.augmentations:\n",
    "                # Create a tensor of angles for the entire batch\n",
    "                angles = torch.tensor(self.random_angles[batch_start:batch_end])\n",
    "                \n",
    "                # Rotate the batch of images and labels\n",
    "                batch_images = kornia.geometry.transform.rotate(batch_images, angles)\n",
    "                batch_organs = kornia.geometry.transform.rotate(batch_organs, angles)\n",
    "                batch_gonogos = kornia.geometry.transform.rotate(batch_gonogos, angles)\n",
    "\n",
    "            images_aug.append(batch_images)\n",
    "            organs_aug.append(batch_organs)\n",
    "            gonogos_aug.append(batch_gonogos)\n",
    "            \n",
    "        # Concatenate all batches into a single tensor\n",
    "        self.images_pretransformed = torch.cat(images)\n",
    "        self.organs_pretransformed = torch.cat(organs)\n",
    "        self.gonogos_pretransformed = torch.cat(gonogos)\n",
    "\n",
    "        self.images_aug = torch.cat(images_aug)\n",
    "        self.organs_aug = torch.cat(organs_aug)\n",
    "        self.gonogos_aug = torch.cat(gonogos_aug)\n",
    "        print('pretransforming data done')\n",
    "\n",
    "    def augment_data(self, batch_size=8):\n",
    "        images = []\n",
    "        organs = []\n",
    "        gonogos = []\n",
    "        for batch_start in tqdm(range(0, len(self.image_files), batch_size)):\n",
    "            batch_end = min(batch_start + batch_size, len(self.image_files))\n",
    "            batch_images = self.images_pretransformed[batch_start:batch_end]\n",
    "            batch_organs = self.organs_pretransformed[batch_start:batch_end]\n",
    "            batch_gonogos = self.gonogos_pretransformed[batch_start:batch_end]\n",
    "\n",
    "            angles = torch.tensor(self.random_angles[batch_start:batch_end])\n",
    "\n",
    "            batch_images = kornia.geometry.transform.rotate(batch_images, angles)\n",
    "            batch_organs = kornia.geometry.transform.rotate(batch_organs, angles)\n",
    "            batch_gonogos = kornia.geometry.transform.rotate(batch_gonogos, angles)\n",
    "\n",
    "            images.append(batch_images)\n",
    "            organs.append(batch_organs)\n",
    "            gonogos.append(batch_gonogos)\n",
    "        \n",
    "        self.images_aug = torch.cat(images)\n",
    "        self.organs_aug = torch.cat(organs)\n",
    "        self.gonogos_aug = torch.cat(gonogos)\n",
    "        \n",
    "    def reset_random_angles(self, seed=None):\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        self.random_angles = self.train_angle_max * (torch.rand(len(self.image_files)) * 2 - 1)\n",
    "        print('angles reset')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        verbose = False\n",
    "        if self.pretransform:\n",
    "            image = self.images_pretransformed[idx]\n",
    "            organ_label = self.organs_pretransformed[idx]\n",
    "            gonogo_label = self.gonogos_pretransformed[idx]\n",
    "        else:\n",
    "            start = time.time()\n",
    "            image_file = os.path.join(self.images_dir, self.image_files[idx])\n",
    "            organ_label_file = os.path.join(self.organ_labels_dir, self.image_files[idx])\n",
    "            gonogo_label_file = os.path.join(self.gonogo_labels_dir, self.image_files[idx])\n",
    "            if verbose:\n",
    "                print('time in data1', time.time() - start)\n",
    "            start = time.time()\n",
    "            # Read image and label\n",
    "            image = Image.open(image_file).convert(\"RGB\")\n",
    "            organ_label = Image.open(organ_label_file).convert(\"L\") # L is grayscale\n",
    "            gonogo_label = Image.open(gonogo_label_file).convert(\"L\")\n",
    "            if verbose:\n",
    "                print('time in data2', time.time() - start)\n",
    "            start = time.time()\n",
    "\n",
    "            image = self.image_transforms(image)\n",
    "            organ_label = self.label_transforms(organ_label)\n",
    "            gonogo_label = self.label_transforms(gonogo_label)\n",
    "            if verbose:\n",
    "                print('time in data3', time.time() - start)\n",
    "            start = time.time()\n",
    "                \n",
    "            if self.augmentations: #self.split.startswith(\"train\"): we should allow choice for using augmentation or not for both train and test\n",
    "                # Apply the random rotation\n",
    "                angle = self.random_angles[idx].item()\n",
    "                if idx == 0:\n",
    "                    print('angle', angle)\n",
    "                image = transforms.functional.rotate(image, angle)\n",
    "                organ_label = transforms.functional.rotate(organ_label, angle)\n",
    "                gonogo_label = transforms.functional.rotate(gonogo_label, angle)\n",
    "\n",
    "        if verbose:\n",
    "            print('time in data4', time.time() - start)\n",
    "        start = time.time()\n",
    "        organ_label = (organ_label * 255).round().long()\n",
    "        gonogo_label = (gonogo_label * 255).round().long()\n",
    "        if verbose:\n",
    "            print('time in data', time.time() - start)\n",
    "        return {\n",
    "            'image': image, \n",
    "            'organs': organ_label,\n",
    "            'gonogo': gonogo_label,\n",
    "            'idx': idx\n",
    "        }\n",
    "    \n",
    "    def get_cv_splits(self, cv_fold=5, gen_seed=0): # need to test\n",
    "        dataset = self\n",
    "        assert cv_fold > 1\n",
    "        assert len(dataset.image_files) > cv_fold\n",
    "\n",
    "        # get a subset of train_ratio split by videos according to a random seed\n",
    "        num_all, num_train = len(dataset.image_files), int(len(dataset.image_files) / cv_fold)\n",
    "        gen = torch.Generator()\n",
    "        gen.manual_seed(gen_seed)\n",
    "        perm = torch.randperm(num_all, generator=gen)\n",
    "        # idxs should be all the folds\n",
    "        idxs = perm[:num_train * cv_fold].reshape(cv_fold, num_train)\n",
    "\n",
    "        # set the new dataset's image_files and video_globs\n",
    "        datasets = []\n",
    "        for i in range(cv_fold):\n",
    "            image_files_new = [dataset.image_files[j] for j in idxs[i]]\n",
    "            new_dataset = copy.deepcopy(dataset)\n",
    "            new_dataset.image_files = image_files_new\n",
    "            datasets.append(new_dataset)\n",
    "        \n",
    "        return datasets\n",
    "\n",
    "    # get a subset\n",
    "    def get_subset(self, split='train', train_ratio=0.9, gen_seed=0):\n",
    "        dataset = self\n",
    "        assert split in ['train', 'val']\n",
    "        video_globs_used = []\n",
    "        used_image_files_all = {}\n",
    "        for i in range(len(dataset.video_globs)):\n",
    "            image_files_curr = [os.path.basename(path) for path in glob.glob(os.path.join(dataset.images_dir, dataset.video_globs[i]))]\n",
    "            used_image_files = sorted(set(image_files_curr) & set(dataset.image_files))\n",
    "            if len(used_image_files) > 0:\n",
    "                used_image_files_all[dataset.video_globs[i]] = used_image_files\n",
    "                video_globs_used.append(dataset.video_globs[i])\n",
    "\n",
    "        # get a subset of train_ratio split by videos according to a random seed\n",
    "        num_all, num_train = len(video_globs_used), int(len(video_globs_used) * train_ratio)\n",
    "        gen = torch.Generator()\n",
    "        gen.manual_seed(gen_seed)\n",
    "        perm = torch.randperm(num_all, generator=gen)\n",
    "        idxs = perm[:num_train] if \"train\" in split else perm[num_train:]\n",
    "\n",
    "        # set the new dataset's image_files and video_globs\n",
    "        image_files_new = []\n",
    "        video_globs_new = []\n",
    "        for i in idxs:\n",
    "            image_files_new += [os.path.basename(path) for path in \\\n",
    "                glob.glob(os.path.join(dataset.images_dir, video_globs_used[i]))]\n",
    "            video_globs_new.append(video_globs_used[i])\n",
    "        image_files_new = sorted(image_files_new)\n",
    "        image_files_idxs = [dataset.image_files.index(image_file) for image_file in image_files_new]\n",
    "        new_dataset = copy.deepcopy(dataset)\n",
    "        new_dataset.image_files = image_files_new\n",
    "        new_dataset.video_globs = video_globs_new\n",
    "\n",
    "        return new_dataset\n",
    "    \n",
    "    def concat_set(self, other):\n",
    "        dataset = copy.deepcopy(self)\n",
    "        dataset.image_files = [filename for filename in self.image_files] + \\\n",
    "            [filename for filename in other.image_files]\n",
    "        dataset.video_globs = sorted(set([filename for filename in self.video_globs] + \\\n",
    "            [filename for filename in other.video_globs]))\n",
    "        dataset.random_angles = torch.cat([self.random_angles, other.random_angles])\n",
    "        return dataset\n",
    "\n",
    "def get_cholecgonogo_datasets(mode, train_test_seed=0, train_val_seed=0, augmentations=False,\n",
    "                            data_dir='/shared_data0/weiqiuy/real_drs/data/abdomen_exlib',\n",
    "                            public_split_filepath_train='/shared_data0/weiqiuy/real_drs/data/splits/public_train0.txt',\n",
    "                            public_split_filepath_test='/shared_data0/weiqiuy/real_drs/data/splits/public_test0.txt',\n",
    "                            private_split_filepath_train=None,\n",
    "                            private_split_filepath_test=None):\n",
    "    assert mode in ['public', 'private', 'all']\n",
    "\n",
    "    # train sets\n",
    "    train_public_dataset = CholecGonogoDataset(data_dir=data_dir, split='train', video_globs='public', \n",
    "        split_filepath=public_split_filepath_train, gen_seed=train_test_seed)\n",
    "    train_private_dataset = CholecGonogoDataset(data_dir=data_dir, split='train', video_globs='private', \n",
    "        split_filepath=private_split_filepath_train, gen_seed=train_test_seed)\n",
    "\n",
    "    # test sets\n",
    "    test_public_dataset = CholecGonogoDataset(data_dir=data_dir, split='test', video_globs='public', \n",
    "        split_filepath=public_split_filepath_test, gen_seed=train_test_seed)\n",
    "    test_private_dataset = CholecGonogoDataset(data_dir=data_dir, split='test', video_globs='private', \n",
    "        split_filepath=private_split_filepath_test, gen_seed=train_test_seed)\n",
    "\n",
    "    # Split train into train and val\n",
    "    train_public_dataset_train = train_public_dataset.get_subset(split='train', gen_seed=train_val_seed)\n",
    "    train_public_dataset_val = train_public_dataset.get_subset(split='val', gen_seed=train_val_seed)\n",
    "    train_private_dataset_train = train_private_dataset.get_subset(split='train', gen_seed=train_val_seed)\n",
    "    train_private_dataset_val = train_private_dataset.get_subset(split='val', gen_seed=train_val_seed)\n",
    "\n",
    "    if mode == 'public':\n",
    "        train_dataset = train_public_dataset_train\n",
    "        val_dataset = train_public_dataset_val\n",
    "        test_dataset = test_public_dataset\n",
    "    elif mode == 'private':\n",
    "        train_dataset = train_private_dataset_train\n",
    "        val_dataset = train_private_dataset_val\n",
    "        test_dataset = test_private_dataset\n",
    "    elif mode == 'all':\n",
    "        train_dataset = train_public_dataset_train.concat_set(train_private_dataset_train)\n",
    "        val_dataset = train_public_dataset_val.concat_set(train_private_dataset_val)\n",
    "        test_dataset = test_public_dataset.concat_set(test_private_dataset)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    return {\n",
    "        'train_dataset': train_dataset,\n",
    "        'val_dataset': val_dataset,\n",
    "        'test_dataset': test_dataset\n",
    "    }\n",
    "\n",
    "# todo: cross validation split # need to test\n",
    "# train/val is cross validation, test is the same\n",
    "def get_cholecgonogo_cv_datasets(mode, cv_fold=5, gen_seed=0, train_test_seed=0, train_val_seed=0, augmentations=False,\n",
    "                            data_dir='/shared_data0/weiqiuy/real_drs/data/abdomen_exlib',\n",
    "                            public_split_filepath_train='/shared_data0/weiqiuy/real_drs/data/splits/public_train0.txt',\n",
    "                            public_split_filepath_test='/shared_data0/weiqiuy/real_drs/data/splits/public_test0.txt',\n",
    "                            private_split_filepath_train=None,\n",
    "                            private_split_filepath_test=None):\n",
    "    assert mode in ['public', 'private', 'all']\n",
    "    # train sets\n",
    "    train_public_dataset = CholecGonogoDataset(data_dir=data_dir, split='train', video_globs='public', \n",
    "        split_filepath=public_split_filepath_train, gen_seed=train_test_seed)\n",
    "    train_private_dataset = CholecGonogoDataset(data_dir=data_dir, split='train', video_globs='private', \n",
    "        split_filepath=private_split_filepath_train, gen_seed=train_test_seed)\n",
    "\n",
    "    # test sets\n",
    "    test_public_dataset = CholecGonogoDataset(data_dir=data_dir, split='test', video_globs='public', \n",
    "        split_filepath=public_split_filepath_test, gen_seed=train_test_seed)\n",
    "    test_private_dataset = CholecGonogoDataset(data_dir=data_dir, split='test', video_globs='private', \n",
    "        split_filepath=private_split_filepath_test, gen_seed=train_test_seed)\n",
    "\n",
    "    # Split train into train and val\n",
    "    train_public_datasets = train_public_dataset.get_cv_splits(cv_fold=cv_fold, gen_seed=gen_seed)\n",
    "    train_private_datasets = train_private_dataset.get_cv_splits(cv_fold=cv_fold, gen_seed=gen_seed)\n",
    "\n",
    "    datasets = []\n",
    "    for i in range(cv_fold):\n",
    "        if mode == 'public':\n",
    "            train_dataset = train_public_datasets[i]\n",
    "            val_dataset = train_public_datasets[(i+1)%cv_fold]\n",
    "            test_dataset = test_public_dataset\n",
    "        elif mode == 'private':\n",
    "            train_dataset = train_private_datasets[i]\n",
    "            val_dataset = train_private_datasets[(i+1)%cv_fold]\n",
    "            test_dataset = test_private_dataset\n",
    "        elif mode == 'all':\n",
    "            train_dataset = train_public_datasets[i].concat_set(train_private_datasets[i])\n",
    "            val_dataset = train_public_datasets[(i+1)%cv_fold].concat_set(train_private_datasets[(i+1)%cv_fold])\n",
    "            test_dataset = test_public_dataset.concat_set(test_private_dataset)\n",
    "        datasets.append({\n",
    "            'train_dataset': train_dataset,\n",
    "            'val_dataset': val_dataset,\n",
    "            'test_dataset': test_dataset\n",
    "        })\n",
    "\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef96cdb-64c1-4c28-b21c-798902cead7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "from collections import namedtuple\n",
    "from typing import Optional, Union, List\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "ModelOutput = namedtuple(\"ModelOutput\", [\"logits\", \"pooler_output\"])\n",
    "\n",
    "\n",
    "class Unet(smp.Unet):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str = \"resnet50\",\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: Optional[str] = \"imagenet\",\n",
    "        decoder_use_batchnorm: bool = True,\n",
    "        decoder_channels: List[int] = (256, 128, 64, 32, 16),\n",
    "        decoder_attention_type: Optional[str] = None,\n",
    "        in_channels: int = 3,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[Union[str, callable]] = None,\n",
    "        aux_params: Optional[dict] = None,\n",
    "    ):\n",
    "        super().__init__(encoder_name=encoder_name,\n",
    "                         encoder_depth=encoder_depth,\n",
    "                         encoder_weights=encoder_weights,\n",
    "                         decoder_use_batchnorm=decoder_use_batchnorm,\n",
    "                         decoder_channels=decoder_channels,\n",
    "                         decoder_attention_type=decoder_attention_type,\n",
    "                         in_channels=in_channels,\n",
    "                         classes=classes,\n",
    "                         activation=activation,\n",
    "                         aux_params=aux_params)\n",
    "\n",
    "    def forward(self, x, return_tuple=False):\n",
    "        \"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\n",
    "\n",
    "        self.check_input_shape(x)\n",
    "\n",
    "        features = self.encoder(x)\n",
    "        decoder_output = self.decoder(*features)\n",
    "\n",
    "        masks = self.segmentation_head(decoder_output)\n",
    "\n",
    "        if self.classification_head is not None:\n",
    "            labels = self.classification_head(features[-1])\n",
    "            return masks, labels\n",
    "\n",
    "        if return_tuple:\n",
    "            return ModelOutput(logits=masks,\n",
    "                               pooler_output=decoder_output)\n",
    "        else:\n",
    "            return masks\n",
    "\n",
    "\n",
    "# Basic classification model\n",
    "class AbdomenClsModel(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 256, kernel_size=3, stride=2, padding=1),   # (N,256,32,32)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, stride=2, padding=1), # (N,128,16,16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=2, padding=1),  # (N,64,8,8)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=2, padding=1),   # (N,32,4,4)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(1),\n",
    "            nn.Linear(32*4*4, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        assert C == 3\n",
    "        x = F.interpolate(x, size=[64,64])\n",
    "        y = self.layers(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "# Basic segmentation model\n",
    "class AbdomenSegModel(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=3,\n",
    "                 encoder_name=\"resnet50\", encoder_weights=\"imagenet\", \n",
    "                 activation=\"softmax2d\"):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.unet = Unet(encoder_name=encoder_name,\n",
    "                             encoder_weights=encoder_weights,\n",
    "                             in_channels=in_channels,\n",
    "                             classes=num_classes,\n",
    "                             activation=activation)\n",
    "\n",
    "    def forward(self, x, return_tuple=True):\n",
    "        N, C, H, W = x.shape\n",
    "        assert H % 32 == 0 and W % 32 == 0\n",
    "        return self.unet(x, return_tuple=return_tuple)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
