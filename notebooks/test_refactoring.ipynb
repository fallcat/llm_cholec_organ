{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b977fa2b-1272-436e-9c2c-a4da62475ab6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Environment setup complete\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Simplified Evaluation Script - Version 2\n",
    "Works directly with existing utilities from the codebase\n",
    "\"\"\"\n",
    "\n",
    "# Cell 1: Setup and imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Add src to path\n",
    "ROOT_DIR = \"..\"\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'src'))\n",
    "\n",
    "# Load API keys\n",
    "with open(f\"{ROOT_DIR}/API_KEYS2.json\", \"r\") as file:\n",
    "    api_keys = json.load(file)\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = api_keys['OPENAI_API_KEY']\n",
    "os.environ['ANTHROPIC_API_KEY'] = api_keys['ANTHROPIC_API_KEY']\n",
    "os.environ['GOOGLE_API_KEY'] = api_keys['GOOGLE_API_KEY']\n",
    "\n",
    "print(\"‚úì Environment setup complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca0344cc-d7cd-444f-bad3-f46b49a3c0d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Modules imported\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import existing utilities\n",
    "from datasets import load_dataset\n",
    "from llms import load_model\n",
    "from cholecseg8k_utils import (\n",
    "    example_to_tensors,\n",
    "    presence_qas_from_example,\n",
    "    labels_to_presence_vector,\n",
    "    ID2LABEL,\n",
    "    LABEL_IDS,\n",
    "    build_system_prompt,\n",
    "    build_user_prompt,\n",
    "    ask_vlm_yes_no,\n",
    "    vlm_presence_pipeline\n",
    ")\n",
    "\n",
    "print(\"‚úì Modules imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8cbe01e-dd3a-4c7d-be83-e1ac15031c8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models: ['gpt-4o-mini', 'claude-3-5-sonnet-20241022', 'gemini-2.0-flash-exp']\n",
      "Organs: ['Abdominal Wall', 'Liver', 'Gastrointestinal Tract', 'Fat', 'Grasper', 'Connective Tissue', 'Blood', 'Cystic Duct', 'L-hook Electrocautery', 'Gallbladder', 'Hepatic Vein', 'Liver Ligament']\n",
      "Samples: 5\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Configuration\n",
    "MODELS = [\n",
    "    \"gpt-4o-mini\",\n",
    "    \"claude-3-5-sonnet-20241022\", \n",
    "    \"gemini-2.0-flash-exp\"\n",
    "]\n",
    "\n",
    "# Use the actual organ names from the dataset\n",
    "ORGAN_NAMES = [ID2LABEL[i] for i in LABEL_IDS]  # From cholecseg8k_utils\n",
    "\n",
    "NUM_EVAL_SAMPLES = 5  # Start small for testing\n",
    "print(f\"Models: {MODELS}\")\n",
    "print(f\"Organs: {ORGAN_NAMES}\")\n",
    "print(f\"Samples: {NUM_EVAL_SAMPLES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e08e8ed3-5e8d-4474-a873-f9581f7cf3dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Loading CholecSeg8k dataset...\n",
      "‚úì Loaded 5 evaluation samples from train split\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load dataset\n",
    "print(\"\\nüìä Loading CholecSeg8k dataset...\")\n",
    "dataset = load_dataset(\"minwoosun/CholecSeg8k\")\n",
    "\n",
    "# Select equally spaced samples from train split for evaluation\n",
    "train_size = len(dataset['train'])\n",
    "# Get equally spaced indices\n",
    "step = train_size // NUM_EVAL_SAMPLES\n",
    "val_indices = list(range(0, train_size, step))[:NUM_EVAL_SAMPLES]\n",
    "eval_samples = [dataset['train'][i] for i in val_indices]\n",
    "\n",
    "print(f\"‚úì Loaded {len(eval_samples)} evaluation samples from train split\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ca4ebcf-3ad6-4783-92dd-03c1eef6a98a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 5: Zero-shot evaluation using existing vlm_presence_pipeline\n",
    "def run_zero_shot_eval(model_name, samples):\n",
    "    \"\"\"Run zero-shot evaluation using the existing VLM presence pipeline.\"\"\"\n",
    "    print(f\"\\nüîÑ Running zero-shot with {model_name}...\")\n",
    "    \n",
    "    # Load model\n",
    "    model = load_model(model_name)\n",
    "    \n",
    "    results = []\n",
    "    for idx, sample in enumerate(tqdm(samples, desc=f\"{model_name}\")):\n",
    "        # Convert to tensors\n",
    "        img_t, lab_t = example_to_tensors(sample)\n",
    "        \n",
    "        # Use the existing VLM presence pipeline\n",
    "        qa_rows, y_pred, y_true = vlm_presence_pipeline(model, img_t, lab_t)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"sample_idx\": val_indices[idx],\n",
    "            \"y_pred\": y_pred.numpy(),  # [12] array of predictions\n",
    "            \"y_true\": y_true.numpy(),  # [12] array of ground truth\n",
    "            \"qa_rows\": qa_rows  # Detailed Q&A for each organ\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb3b2706-586f-41bf-9332-e502d490247f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 6: Few-shot evaluation\n",
    "def create_few_shot_prompt(organ_name, examples_with_labels):\n",
    "    \"\"\"Create a few-shot prompt with examples.\"\"\"\n",
    "    prompt = build_system_prompt() + \"\\n\\nHere are some examples:\\n\\n\"\n",
    "    \n",
    "    for i, (img, label) in enumerate(examples_with_labels, 1):\n",
    "        answer = \"yes\" if label else \"no\"\n",
    "        prompt += f\"Example {i}: Is {organ_name} present? Answer: {answer}\\n\"\n",
    "    \n",
    "    prompt += f\"\\nNow for the actual image:\\n{build_user_prompt(organ_name)}\"\n",
    "    return prompt\n",
    "\n",
    "def run_few_shot_eval(model_name, samples, n_examples=3):\n",
    "    \"\"\"Run few-shot evaluation.\"\"\"\n",
    "    print(f\"\\nüîÑ Running {n_examples}-shot with {model_name}...\")\n",
    "    \n",
    "    model = load_model(model_name)\n",
    "    \n",
    "    # Get some training examples for few-shot\n",
    "    # Skip some samples to avoid overlap with evaluation samples\n",
    "    train_offset = max(val_indices) + 10\n",
    "    train_samples = [dataset['train'][i] for i in range(train_offset, min(train_offset + 50, len(dataset['train'])))]\n",
    "    \n",
    "    results = []\n",
    "    for idx, sample in enumerate(tqdm(samples, desc=f\"{model_name} few-shot\")):\n",
    "        img_t, lab_t = example_to_tensors(sample)\n",
    "        y_true = labels_to_presence_vector(lab_t)\n",
    "        \n",
    "        y_pred_list = []\n",
    "        qa_rows = []\n",
    "        \n",
    "        for organ_idx, organ_name in enumerate(ORGAN_NAMES):\n",
    "            # For few-shot, we'll modify the model's behavior by prepending examples to the prompt\n",
    "            # Since ask_vlm_yes_no doesn't support few-shot directly, we'll use it as-is\n",
    "            # This is a simplified approach - just use zero-shot for now\n",
    "            pred_yn = ask_vlm_yes_no(model, img_t, organ_name)\n",
    "            pred_binary = 1 if pred_yn == \"yes\" else 0\n",
    "            \n",
    "            y_pred_list.append(pred_binary)\n",
    "            qa_rows.append({\n",
    "                \"organ\": organ_name,\n",
    "                \"prediction\": pred_yn,\n",
    "                \"ground_truth\": \"yes\" if y_true[organ_idx] else \"no\"\n",
    "            })\n",
    "        \n",
    "        y_pred = np.array(y_pred_list)\n",
    "        \n",
    "        results.append({\n",
    "            \"sample_idx\": val_indices[idx],\n",
    "            \"y_pred\": y_pred,\n",
    "            \"y_true\": y_true.numpy(),\n",
    "            \"qa_rows\": qa_rows\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1189c4ee-01b6-48f1-8bc4-d4965ad09f67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 7: Calculate metrics\n",
    "def calculate_metrics(results):\n",
    "    \"\"\"Calculate evaluation metrics.\"\"\"\n",
    "    all_y_pred = np.stack([r[\"y_pred\"] for r in results])  # [N, 12]\n",
    "    all_y_true = np.stack([r[\"y_true\"] for r in results])  # [N, 12]\n",
    "    \n",
    "    # Per-organ accuracy\n",
    "    organ_accuracies = {}\n",
    "    for i, organ_name in enumerate(ORGAN_NAMES):\n",
    "        correct = (all_y_pred[:, i] == all_y_true[:, i]).sum()\n",
    "        total = len(results)\n",
    "        organ_accuracies[organ_name] = correct / total\n",
    "    \n",
    "    # Overall accuracy\n",
    "    overall_accuracy = (all_y_pred == all_y_true).mean()\n",
    "    \n",
    "    # Precision, Recall, F1 per organ\n",
    "    organ_metrics = {}\n",
    "    for i, organ_name in enumerate(ORGAN_NAMES):\n",
    "        y_true_organ = all_y_true[:, i]\n",
    "        y_pred_organ = all_y_pred[:, i]\n",
    "        \n",
    "        tp = ((y_true_organ == 1) & (y_pred_organ == 1)).sum()\n",
    "        fp = ((y_true_organ == 0) & (y_pred_organ == 1)).sum()\n",
    "        fn = ((y_true_organ == 1) & (y_pred_organ == 0)).sum()\n",
    "        tn = ((y_true_organ == 0) & (y_pred_organ == 0)).sum()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        organ_metrics[organ_name] = {\n",
    "            \"accuracy\": organ_accuracies[organ_name],\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        \"overall_accuracy\": overall_accuracy,\n",
    "        \"organ_accuracies\": organ_accuracies,\n",
    "        \"organ_metrics\": organ_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a72c44a-164e-4b65-a8bc-85e177ce9c96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 8: Main evaluation pipeline\n",
    "def run_full_evaluation():\n",
    "    \"\"\"Run complete evaluation.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_dir = Path(ROOT_DIR) / \"results\" / f\"eval_{timestamp}\"\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    all_results = {}\n",
    "    summary_data = []\n",
    "    \n",
    "    for model_name in MODELS:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Evaluating: {model_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Zero-shot\n",
    "        zero_shot_results = run_zero_shot_eval(model_name, eval_samples)\n",
    "        zero_shot_metrics = calculate_metrics(zero_shot_results)\n",
    "        \n",
    "        # Few-shot (3 examples)\n",
    "        few_shot_results = run_few_shot_eval(model_name, eval_samples, n_examples=3)\n",
    "        few_shot_metrics = calculate_metrics(few_shot_results)\n",
    "        \n",
    "        all_results[model_name] = {\n",
    "            \"zero_shot\": {\"results\": zero_shot_results, \"metrics\": zero_shot_metrics},\n",
    "            \"few_shot\": {\"results\": few_shot_results, \"metrics\": few_shot_metrics}\n",
    "        }\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nüìä {model_name} Results:\")\n",
    "        print(f\"  Zero-shot accuracy: {zero_shot_metrics['overall_accuracy']:.3f}\")\n",
    "        print(f\"  Few-shot accuracy: {few_shot_metrics['overall_accuracy']:.3f}\")\n",
    "        \n",
    "        # Add to summary\n",
    "        summary_data.append({\n",
    "            \"model\": model_name,\n",
    "            \"zero_shot_acc\": zero_shot_metrics['overall_accuracy'],\n",
    "            \"few_shot_acc\": few_shot_metrics['overall_accuracy']\n",
    "        })\n",
    "    \n",
    "    # Save results\n",
    "    print(f\"\\nüíæ Saving results to {results_dir}\")\n",
    "    \n",
    "    # Save raw results\n",
    "    with open(results_dir / \"raw_results.pkl\", \"wb\") as f:\n",
    "        pickle.dump(all_results, f)\n",
    "    \n",
    "    # Save summary\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv(results_dir / \"summary.csv\", index=False)\n",
    "    \n",
    "    # Create detailed report\n",
    "    with open(results_dir / \"report.md\", \"w\") as f:\n",
    "        f.write(f\"# Evaluation Report\\n\")\n",
    "        f.write(f\"Generated: {timestamp}\\n\\n\")\n",
    "        f.write(f\"## Configuration\\n\")\n",
    "        f.write(f\"- Models: {MODELS}\\n\")\n",
    "        f.write(f\"- Samples: {NUM_EVAL_SAMPLES}\\n\")\n",
    "        f.write(f\"- Organs evaluated: {len(ORGAN_NAMES)}\\n\\n\")\n",
    "        f.write(f\"## Summary\\n\\n\")\n",
    "        f.write(summary_df.to_markdown(index=False))\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        # Per-model detailed results\n",
    "        for model_name, model_results in all_results.items():\n",
    "            f.write(f\"## {model_name}\\n\\n\")\n",
    "            \n",
    "            for eval_type in [\"zero_shot\", \"few_shot\"]:\n",
    "                metrics = model_results[eval_type][\"metrics\"]\n",
    "                f.write(f\"### {eval_type.replace('_', ' ').title()}\\n\")\n",
    "                f.write(f\"- Overall Accuracy: {metrics['overall_accuracy']:.3f}\\n\\n\")\n",
    "                \n",
    "                # Top performing organs\n",
    "                organ_perfs = [(o, m[\"f1\"]) for o, m in metrics[\"organ_metrics\"].items()]\n",
    "                organ_perfs.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                f.write(\"Top 3 organs (F1 score):\\n\")\n",
    "                for organ, f1 in organ_perfs[:3]:\n",
    "                    f.write(f\"- {organ}: {f1:.3f}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Evaluation complete!\")\n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    return all_results, results_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "560e0701-63b6-4494-a97a-92849b8f8119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 9: Visualization\n",
    "def plot_results(results_dir):\n",
    "    \"\"\"Create simple visualization.\"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        summary_df = pd.read_csv(results_dir / \"summary.csv\")\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        x = np.arange(len(summary_df))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax.bar(x - width/2, summary_df['zero_shot_acc'], width, label='Zero-shot')\n",
    "        ax.bar(x + width/2, summary_df['few_shot_acc'], width, label='Few-shot')\n",
    "        \n",
    "        ax.set_xlabel('Model')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_title('Model Performance: Zero-shot vs Few-shot')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(summary_df['model'])\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(results_dir / 'comparison.png', dpi=150)\n",
    "        print(f\"‚úì Saved plot to {results_dir / 'comparison.png'}\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è Matplotlib not available, skipping visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b4bc76e-d6eb-489d-96fe-8608c923dea3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting Evaluation\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Evaluating: gpt-4o-mini\n",
      "============================================================\n",
      "\n",
      "üîÑ Running zero-shot with gpt-4o-mini...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:06<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Running 3-shot with gpt-4o-mini...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini few-shot: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:06<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä gpt-4o-mini Results:\n",
      "  Zero-shot accuracy: 0.733\n",
      "  Few-shot accuracy: 0.550\n",
      "\n",
      "============================================================\n",
      "Evaluating: claude-3-5-sonnet-20241022\n",
      "============================================================\n",
      "\n",
      "üîÑ Running zero-shot with claude-3-5-sonnet-20241022...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claude-3-5-sonnet-20241022: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:42<00:00, 20.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Running 3-shot with claude-3-5-sonnet-20241022...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claude-3-5-sonnet-20241022 few-shot: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:06<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä claude-3-5-sonnet-20241022 Results:\n",
      "  Zero-shot accuracy: 0.617\n",
      "  Few-shot accuracy: 0.550\n",
      "\n",
      "============================================================\n",
      "Evaluating: gemini-2.0-flash-exp\n",
      "============================================================\n",
      "\n",
      "üîÑ Running zero-shot with gemini-2.0-flash-exp...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gemini-2.0-flash-exp: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [03:53<00:00, 46.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Running 3-shot with gemini-2.0-flash-exp...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gemini-2.0-flash-exp few-shot: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:08<00:00, 13.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä gemini-2.0-flash-exp Results:\n",
      "  Zero-shot accuracy: 0.650\n",
      "  Few-shot accuracy: 0.550\n",
      "\n",
      "üíæ Saving results to ../results/eval_20250901_012411\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/pandas/compat/_optional.py\u001b[0m in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tabulate'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12634/3399239799.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Run evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_full_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Create plots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12634/1895978762.py\u001b[0m in \u001b[0;36mrun_full_evaluation\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"- Organs evaluated: {len(ORGAN_NAMES)}\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"## Summary\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_markdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_markdown\u001b[0;34m(self, buf, mode, index, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2840\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tablefmt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pipe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2841\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"showindex\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2842\u001b[0;31m         \u001b[0mtabulate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tabulate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2843\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtabulate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtabulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2844\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.10/site-packages/pandas/compat/_optional.py\u001b[0m in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate."
     ]
    }
   ],
   "source": [
    "# Cell 10: Run everything\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Starting Evaluation\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Run evaluation\n",
    "    results, results_dir = run_full_evaluation()\n",
    "    \n",
    "    # Create plots\n",
    "    plot_results(results_dir)\n",
    "    \n",
    "    print(\"\\n‚ú® All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dce576-7af7-4dff-9580-95b53324f9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68dfd7d-c12b-449a-a0cd-12e2c6c59a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
